
@misc{basile_llamantino_2023,
	title = {{LLaMAntino}: {LLaMA} 2 Models for Effective Text Generation in Italian Language},
	url = {http://arxiv.org/abs/2312.09993},
	doi = {10.48550/arXiv.2312.09993},
	shorttitle = {{LLaMAntino}},
	abstract = {Large Language Models represent state-of-the-art linguistic models designed to equip computers with the ability to comprehend natural language. With its exceptional capacity to capture complex contextual relationships, the {LLaMA} (Large Language Model Meta {AI}) family represents a novel advancement in the field of natural language processing by releasing foundational models designed to improve the natural language understanding abilities of the transformer architecture thanks to their large amount of trainable parameters (7, 13, and 70 billion parameters). In many natural language understanding tasks, these models obtain the same performances as private company models such as {OpenAI} Chat-{GPT} with the advantage to make publicly available weights and code for research and commercial uses. In this work, we investigate the possibility of Language Adaptation for {LLaMA} models, explicitly focusing on addressing the challenge of Italian Language coverage. Adopting an open science approach, we explore various tuning approaches to ensure a high-quality text generated in Italian suitable for common tasks in this underrepresented language in the original models' datasets. We aim to release effective text generation models with strong linguistic properties for many tasks that seem challenging using multilingual or general-purpose {LLMs}. By leveraging an open science philosophy, this study contributes to Language Adaptation strategies for the Italian language by introducing the novel {LLaMAntino} family of Italian {LLMs}.},
	number = {{arXiv}:2312.09993},
	publisher = {{arXiv}},
	author = {Basile, Pierpaolo and Musacchio, Elio and Polignano, Marco and Siciliani, Lucia and Fiameni, Giuseppe and Semeraro, Giovanni},
	urldate = {2024-12-10},
	date = {2023-12-15},
	eprinttype = {arxiv},
	eprint = {2312.09993 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\aless\\Zotero\\storage\\XLZ5U33K\\Basile et al. - 2023 - LLaMAntino LLaMA 2 Models for Effective Text Generation in Italian Language.pdf:application/pdf;Snapshot:C\:\\Users\\aless\\Zotero\\storage\\CUDFISCT\\2312.html:text/html},
}

@misc{magesh_hallucination-free_2024,
	title = {Hallucination-Free? Assessing the Reliability of Leading {AI} Legal Research Tools},
	url = {http://arxiv.org/abs/2405.20362},
	doi = {10.48550/arXiv.2405.20362},
	shorttitle = {Hallucination-Free?},
	abstract = {Legal practice has witnessed a sharp rise in products incorporating artificial intelligence ({AI}). Such tools are designed to assist with a wide range of core legal tasks, from search and summarization of caselaw to document drafting. But the large language models used in these tools are prone to "hallucinate," or make up false information, making their use risky in high-stakes domains. Recently, certain legal research providers have touted methods such as retrieval-augmented generation ({RAG}) as "eliminating" (Casetext, 2023) or "avoid[ing]" hallucinations (Thomson Reuters, 2023), or guaranteeing "hallucination-free" legal citations ({LexisNexis}, 2023). Because of the closed nature of these systems, systematically assessing these claims is challenging. In this article, we design and report on the first preregistered empirical evaluation of {AI}-driven legal research tools. We demonstrate that the providers' claims are overstated. While hallucinations are reduced relative to general-purpose chatbots ({GPT}-4), we find that the {AI} research tools made by {LexisNexis} (Lexis+ {AI}) and Thomson Reuters (Westlaw {AI}-Assisted Research and Ask Practical Law {AI}) each hallucinate between 17\% and 33\% of the time. We also document substantial differences between systems in responsiveness and accuracy. Our article makes four key contributions. It is the first to assess and report the performance of {RAG}-based proprietary legal {AI} tools. Second, it introduces a comprehensive, preregistered dataset for identifying and understanding vulnerabilities in these systems. Third, it proposes a clear typology for differentiating between hallucinations and accurate legal responses. Last, it provides evidence to inform the responsibilities of legal professionals in supervising and verifying {AI} outputs, which remains a central open question for the responsible integration of {AI} into law.},
	number = {{arXiv}:2405.20362},
	publisher = {{arXiv}},
	author = {Magesh, Varun and Surani, Faiz and Dahl, Matthew and Suzgun, Mirac and Manning, Christopher D. and Ho, Daniel E.},
	urldate = {2024-10-04},
	date = {2024-05-30},
	eprinttype = {arxiv},
	eprint = {2405.20362 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:C\:\\Users\\aless\\Zotero\\storage\\A4FTR9N6\\Magesh et al. - 2024 - Hallucination-Free Assessing the Reliability of Leading AI Legal Research Tools.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\aless\\Zotero\\storage\\WJKH4QHV\\2405.html:text/html},
}

@misc{phan_rag_2024,
	title = {{RAG} vs. Long Context: Examining Frontier Large Language Models for Environmental Review Document Comprehension},
	url = {http://arxiv.org/abs/2407.07321},
	doi = {10.48550/arXiv.2407.07321},
	shorttitle = {{RAG} vs. Long Context},
	abstract = {Large Language Models ({LLMs}) have been applied to many research problems across various domains. One of the applications of {LLMs} is providing question-answering systems that cater to users from different fields. The effectiveness of {LLM}-based question-answering systems has already been established at an acceptable level for users posing questions in popular and public domains such as trivia and literature. However, it has not often been established in niche domains that traditionally require specialized expertise. To this end, we construct the {NEPAQuAD}1.0 benchmark to evaluate the performance of three frontier {LLMs} -- Claude Sonnet, Gemini, and {GPT}-4 -- when answering questions originating from Environmental Impact Statements prepared by U.S. federal government agencies in accordance with the National Environmental Environmental Act ({NEPA}). We specifically measure the ability of {LLMs} to understand the nuances of legal, technical, and compliance-related information present in {NEPA} documents in different contextual scenarios. For example, we test the {LLMs}' internal prior {NEPA} knowledge by providing questions without any context, as well as assess how {LLMs} synthesize the contextual information present in long {NEPA} documents to facilitate the question/answering task. We compare the performance of the long context {LLMs} and {RAG} powered models in handling different types of questions (e.g., problem-solving, divergent). Our results suggest that {RAG} powered models significantly outperform the long context models in the answer accuracy regardless of the choice of the frontier {LLM}. Our further analysis reveals that many models perform better answering closed questions than divergent and problem-solving questions.},
	number = {{arXiv}:2407.07321},
	publisher = {{arXiv}},
	author = {Phan, Hung and Acharya, Anurag and Chaturvedi, Sarthak and Sharma, Shivam and Parker, Mike and Nally, Dan and Jannesari, Ali and Pazdernik, Karl and Halappanavar, Mahantesh and Munikoti, Sai and Horawalavithana, Sameera},
	urldate = {2024-10-04},
	date = {2024-07-09},
	eprinttype = {arxiv},
	eprint = {2407.07321 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\aless\\Zotero\\storage\\BUTB7ZT8\\Phan et al. - 2024 - RAG vs. Long Context Examining Frontier Large Language Models for Environmental Review Document Com.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\aless\\Zotero\\storage\\Y3A5YJ2D\\2407.html:text/html},
}

@misc{wei_emergent_2022,
	title = {Emergent Abilities of Large Language Models},
	url = {http://arxiv.org/abs/2206.07682},
	abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
	number = {{arXiv}:2206.07682},
	author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
	urldate = {2024-06-14},
	date = {2022-10-26},
	eprinttype = {arxiv},
	eprint = {2206.07682 [cs]},
	file = {Wei et al. - 2022 - Emergent Abilities of Large Language Models.pdf:C\:\\Users\\aless\\Zotero\\storage\\8KIGF3TP\\Wei et al. - 2022 - Emergent Abilities of Large Language Models.pdf:application/pdf},
}

@misc{liu_lost_2023,
	title = {Lost in the Middle: How Language Models Use Long Contexts},
	url = {http://arxiv.org/abs/2307.03172},
	doi = {10.48550/arXiv.2307.03172},
	shorttitle = {Lost in the Middle},
	abstract = {While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.},
	number = {{arXiv}:2307.03172},
	publisher = {{arXiv}},
	author = {Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
	urldate = {2024-06-14},
	date = {2023-11-20},
	eprinttype = {arxiv},
	eprint = {2307.03172 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\aless\\Zotero\\storage\\CMUI3H8B\\Liu et al. - 2023 - Lost in the Middle How Language Models Use Long C.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\aless\\Zotero\\storage\\W83TY89W\\2307.html:text/html},
}

@article{li_pre-trained_2024,
	title = {Pre-Trained Language Models for Text Generation: A Survey},
	volume = {56},
	issn = {0360-0300},
	url = {https://dl.acm.org/doi/10.1145/3649449},
	doi = {10.1145/3649449},
	shorttitle = {Pre-Trained Language Models for Text Generation},
	abstract = {Text Generation aims to produce plausible and readable text in human language from input data. The resurgence of deep learning has greatly advanced this field, in particular, with the help of neural generation models based on pre-trained language models ({PLMs}). Text generation based on {PLMs} is viewed as a promising approach in both academia and industry. In this article, we provide a survey on the utilization of {PLMs} in text generation. We begin with introducing two key aspects of applying {PLMs} to text generation: (1) how to design an effective {PLM} to serve as the generation model; and (2) how to effectively optimize {PLMs} given the reference text and to ensure that the generated texts satisfy special text properties. Then, we show the major challenges that have arisen in these aspects, as well as possible solutions for them. We also include a summary of various useful resources and typical text generation applications based on {PLMs}. Finally, we highlight the future research directions which will further improve these {PLMs} for text generation. This comprehensive survey is intended to help researchers interested in text generation problems to learn the core concepts, the main techniques and the latest developments in this area based on {PLMs}.},
	pages = {230:1--230:39},
	number = {9},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Li, Junyi and Tang, Tianyi and Zhao, Wayne Xin and Nie, Jian-Yun and Wen, Ji-Rong},
	urldate = {2024-06-14},
	date = {2024-04-25},
	keywords = {natural language processing, Pre-trained language models},
	file = {Full Text PDF:C\:\\Users\\aless\\Zotero\\storage\\67AISLNL\\Li et al. - 2024 - Pre-Trained Language Models for Text Generation A.pdf:application/pdf},
}

@misc{bsharat_principled_2024,
	title = {Principled Instructions Are All You Need for Questioning {LLaMA}-1/2, {GPT}-3.5/4},
	url = {http://arxiv.org/abs/2312.16171},
	doi = {10.48550/arXiv.2312.16171},
	abstract = {This paper introduces 26 guiding principles designed to streamline the process of querying and prompting large language models. Our goal is to simplify the underlying concepts of formulating questions for various scales of large language models, examining their abilities, and enhancing user comprehension on the behaviors of different scales of large language models when feeding into different prompts. Extensive experiments are conducted on {LLaMA}-1/2 (7B, 13B and 70B), {GPT}-3.5/4 to verify the effectiveness of the proposed principles on instructions and prompts design. We hope that this work can provide a better guide for researchers working on the prompting of large language models. Project page is available at https://github.com/{VILA}-Lab/{ATLAS}.},
	number = {{arXiv}:2312.16171},
	publisher = {{arXiv}},
	author = {Bsharat, Sondos Mahmoud and Myrzakhan, Aidar and Shen, Zhiqiang},
	urldate = {2024-06-11},
	date = {2024-01-18},
	eprinttype = {arxiv},
	eprint = {2312.16171 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\aless\\Zotero\\storage\\JMUVIPC3\\Bsharat et al. - 2024 - Principled Instructions Are All You Need for Quest.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\aless\\Zotero\\storage\\U6T6FHBF\\2312.html:text/html},
}

@misc{vaswani_attention_2023,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	number = {{arXiv}:1706.03762},
	publisher = {{arXiv}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2025-03-21},
	date = {2023-08-02},
	eprinttype = {arxiv},
	eprint = {1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\aless\\Zotero\\storage\\AJW98GX4\\1706.html:text/html;Preprint PDF:C\:\\Users\\aless\\Zotero\\storage\\Q43B89XU\\Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;Snapshot:C\:\\Users\\aless\\Zotero\\storage\\DTDGY69T\\1706.html:text/html},
}

@misc{frankenreiter_natural_2022,
	location = {Rochester, {NY}},
	title = {Natural Language Processing in Legal Tech},
	url = {https://papers.ssrn.com/abstract=4027030},
	doi = {10.2139/ssrn.4027030},
	abstract = {Natural language processing techniques promise to automate an activity that lies at the core of many tasks performed by lawyers, namely the extraction and processing of information from unstructured text. The relevant methods are a key ingredient for both current and future legal tech applications, and their potential and limitations will be crucial in determining the extent to which legal tech will succeed in its quest to revolutionize the market for legal services. This chapter provides a non-technical introduction to a selection of natural language processing techniques that are expected to play a major role in legal tech. In addition, it critically discusses the promises and pitfalls of natural language processing tools in this context, using technology-assisted review in discovery and case outcome predictions as examples.},
	number = {4027030},
	publisher = {Social Science Research Network},
	author = {Frankenreiter, Jens and Nyarko, Julian},
	urldate = {2025-03-27},
	date = {2022-02-04},
	langid = {english},
	keywords = {natural language processing, nlp, legal tech},
	file = {Full Text PDF:C\:\\Users\\aless\\Zotero\\storage\\CC7HQQ2B\\Frankenreiter e Nyarko - 2022 - Natural Language Processing in Legal Tech.pdf:application/pdf},
}

@misc{polo_legalnlp_2021,
	title = {{LegalNLP} -- Natural Language Processing methods for the Brazilian Legal Language},
	url = {http://arxiv.org/abs/2110.15709},
	doi = {10.48550/arXiv.2110.15709},
	abstract = {We present and make available pre-trained language models (Phraser, Word2Vec, Doc2Vec, {FastText}, and {BERT}) for the Brazilian legal language, a Python package with functions to facilitate their use, and a set of demonstrations/tutorials containing some applications involving them. Given that our material is built upon legal texts coming from several Brazilian courts, this initiative is extremely helpful for the Brazilian legal field, which lacks other open and specific tools and language models. Our main objective is to catalyze the use of natural language processing tools for legal texts analysis by the Brazilian industry, government, and academia, providing the necessary tools and accessible material.},
	number = {{arXiv}:2110.15709},
	publisher = {{arXiv}},
	author = {Polo, Felipe Maia and Mendonça, Gabriel Caiaffa Floriano and Parreira, Kauê Capellato J. and Gianvechio, Lucka and Cordeiro, Peterson and Ferreira, Jonathan Batista and Lima, Leticia Maria Paz de and Maia, Antônio Carlos do Amaral and Vicente, Renato},
	urldate = {2025-03-27},
	date = {2021-10-05},
	eprinttype = {arxiv},
	eprint = {2110.15709 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\aless\\Zotero\\storage\\72GQ4XLC\\Polo et al. - 2021 - LegalNLP -- Natural Language Processing methods for the Brazilian Legal Language.pdf:application/pdf;Snapshot:C\:\\Users\\aless\\Zotero\\storage\\RYYX6YJG\\2110.html:text/html},
}

@article{quevedo_legal_2024,
	title = {Legal Natural Language Processing From 2015 to 2022: A Comprehensive Systematic Mapping Study of Advances and Applications},
	volume = {12},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/abstract/document/10320368},
	doi = {10.1109/ACCESS.2023.3333946},
	shorttitle = {Legal Natural Language Processing From 2015 to 2022},
	abstract = {The surge in legal text production has amplified the workload for legal professionals, making many tasks repetitive and time-consuming. Furthermore, the complexity and specialized language of legal documents pose challenges not just for those in the legal domain but also for the general public. This emphasizes the potential role and impact of Legal Natural Language Processing (Legal {NLP}). Although advancements have been made in this domain, particularly after 2015 with the advent of Deep Learning and Large Language Models ({LLMs}), a systematic exploration of this progress until 2022 is nonexistent. In this research, we perform a Systematic Mapping Study ({SMS}) to bridge this gap. We aim to provide a descriptive statistical analysis of the Legal {NLP} research between 2015 and 2022. Categorize and sub-categorize primary publications based on their research problems. Identify limitations and areas of improvement in current research. Using a robust search methodology across four reputable indexers, we filtered 536 papers down to 75 pivotal articles. Our findings reveal the diverse methods employed for tasks such as Multiclass Classification, Summarization, and Question Answering in the Legal {NLP} field. We also highlight resources, challenges, and gaps in current methodologies and emphasize the need for curated datasets, ontologies, and a focus on inherent difficulties like data accessibility. As the legal sector gradually embraces Natural Language Processing ({NLP}), understanding the capabilities and limitations of Legal {NLP} becomes vital for ensuring efficient and ethical application. The research offers insights for both Legal {NLP} researchers and the broader legal community, advocating for continued advancements in automation while also addressing ethical concerns.},
	pages = {145286--145317},
	journaltitle = {{IEEE} Access},
	author = {Quevedo, Ernesto and Cerny, Tomas and Rodriguez, Alejandro and Rivas, Pablo and Yero, Jorge and Sooksatra, Korn and Zhakubayev, Alibek and Taibi, Davide},
	urldate = {2025-03-27},
	date = {2024},
	note = {Conference Name: {IEEE} Access},
	keywords = {deep learning, Deep learning, Information retrieval, Law, legal-{NLP}, Natural language processing, Search problems, Surveys, Systematic-mapping-study, Systematics, Task analysis},
	file = {Full Text PDF:C\:\\Users\\aless\\Zotero\\storage\\SBJMU7H8\\Quevedo et al. - 2024 - Legal Natural Language Processing From 2015 to 2022 A Comprehensive Systematic Mapping Study of Adv.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\aless\\Zotero\\storage\\BZVZQN5Z\\10320368.html:text/html},
}

@article{mumcuoglu_natural_2021,
	title = {Natural language processing in law: Prediction of outcomes in the higher courts of Turkey},
	volume = {58},
	issn = {03064573},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0306457321001692},
	doi = {10.1016/j.ipm.2021.102684},
	shorttitle = {Natural language processing in law},
	abstract = {Natural language processing ({NLP}) based approaches have recently received attention for legal systems of several countries. It is of interest to study the wide variety of legal systems that have so far not received any attention. In particular, for the legal system of the Republic of Turkey, codified in Turkish, no works have been published. We first review the state-of-the-art of {NLP} in law, and then study the problem of predicting verdicts for several different courts, using several different algorithms. This study is much broader than earlier studies in the number of different courts and the variety of algorithms it includes. Therefore it provides a reference point and baseline for further studies in this area. We further hope the scope and systematic nature of this study can set a framework that can be applied to the study of other legal systems. We present novel results on predicting the rulings of the Turkish Constitutional Court and Courts of Appeal, using only fact descriptions, and without seeing the actual rulings. The methods that are utilized are based on Decision Trees ({DTs}), Random Forests ({RFs}), Support Vector Machines ({SVMs}) and state-of-the-art deep learning ({DL}) methods; specifically Gated Recurrent Units ({GRUs}), Long Short-Term Memory networks ({LSTMs}) and bidirectional {LSTMs} ({BiLSTMs}), with the integration of an attention mechanism for each model. The prediction results for all algorithms are given in a comparative and detailed manner. We demonstrate that outcomes of the courts of Turkish legal system can be predicted with high accuracy, especially with deep learning based methods. The presented results exhibit similar performance to earlier work in the literature for other languages and legal systems.},
	pages = {102684},
	number = {5},
	journaltitle = {Information Processing \& Management},
	shortjournal = {Information Processing \& Management},
	author = {Mumcuoğlu, Emre and Öztürk, Ceyhun E. and Ozaktas, Haldun M. and Koç, Aykut},
	urldate = {2025-03-27},
	date = {2021-09},
	langid = {english},
	file = {PDF:C\:\\Users\\aless\\Zotero\\storage\\95QP8AHB\\Mumcuoğlu et al. - 2021 - Natural language processing in law Prediction of outcomes in the higher courts of Turkey.pdf:application/pdf},
}

@misc{ariai_natural_2025,
	title = {Natural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges},
	url = {http://arxiv.org/abs/2410.21306},
	doi = {10.48550/arXiv.2410.21306},
	shorttitle = {Natural Language Processing for the Legal Domain},
	abstract = {Natural Language Processing ({NLP}) is revolutionising the way legal professionals and laypersons operate in the legal field. The considerable potential for {NLP} in the legal sector, especially in developing computational tools for various legal processes, has captured the interest of researchers for years. This survey follows the Preferred Reporting Items for Systematic Reviews and Meta-Analyses framework, reviewing 154 studies, with a final selection of 133 after manual filtering. It explores foundational concepts related to {NLP} in the legal domain, illustrating the unique aspects and challenges of processing legal texts, such as extensive document length, complex language, and limited open legal datasets. We provide an overview of {NLP} tasks specific to legal text, such as Legal Document Summarisation, legal Named Entity Recognition, Legal Question Answering, Legal Argument Mining, Legal Text Classification, and Legal Judgement Prediction. In the section on legal Language Models ({LMs}), we analyse both developed {LMs} and approaches for adapting general {LMs} to the legal domain. Additionally, we identify 16 Open Research Challenges, including bias in Artificial Intelligence applications, the need for more robust and interpretable models, and improving explainability to handle the complexities of legal language and reasoning.},
	number = {{arXiv}:2410.21306},
	publisher = {{arXiv}},
	author = {Ariai, Farid and Demartini, Gianluca},
	urldate = {2025-03-27},
	date = {2025-03-25},
	eprinttype = {arxiv},
	eprint = {2410.21306 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\aless\\Zotero\\storage\\UHU8ZCWB\\Ariai e Demartini - 2025 - Natural Language Processing for the Legal Domain A Survey of Tasks, Datasets, Models, and Challenge.pdf:application/pdf;Snapshot:C\:\\Users\\aless\\Zotero\\storage\\GTB2XDY4\\2410.html:text/html},
}

@misc{katz_natural_2023,
	title = {Natural Language Processing in the Legal Domain},
	url = {http://arxiv.org/abs/2302.12039},
	doi = {10.48550/arXiv.2302.12039},
	abstract = {In this paper, we summarize the current state of the field of {NLP} \& Law with a specific focus on recent technical and substantive developments. To support our analysis, we construct and analyze a nearly complete corpus of more than six hundred {NLP} \& Law related papers published over the past decade. Our analysis highlights several major trends. Namely, we document an increasing number of papers written, tasks undertaken, and languages covered over the course of the past decade. We observe an increase in the sophistication of the methods which researchers deployed in this applied context. Slowly but surely, Legal {NLP} is beginning to match not only the methodological sophistication of general {NLP} but also the professional standards of data availability and code reproducibility observed within the broader scientific community. We believe all of these trends bode well for the future of the field, but many questions in both the academic and commercial sphere still remain open.},
	number = {{arXiv}:2302.12039},
	publisher = {{arXiv}},
	author = {Katz, Daniel Martin and Hartung, Dirk and Gerlach, Lauritz and Jana, Abhik and {II}, Michael J. Bommarito},
	urldate = {2025-03-27},
	date = {2023-02-23},
	eprinttype = {arxiv},
	eprint = {2302.12039 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\aless\\Zotero\\storage\\VWN629BC\\Katz et al. - 2023 - Natural Language Processing in the Legal Domain.pdf:application/pdf;Snapshot:C\:\\Users\\aless\\Zotero\\storage\\Q9DZFQAX\\2302.html:text/html},
}

@book{oecd_government_2023,
	title = {Government at a Glance 2023},
	isbn = {978-92-64-67279-6 978-92-64-80350-3 978-92-64-85180-1},
	url = {https://www.oecd.org/en/publications/government-at-a-glance-2023_3d5c5d31-en.html},
	series = {Government at a Glance},
	publisher = {{OECD}},
	author = {{OECD}},
	urldate = {2025-03-27},
	date = {2023-06-30},
	langid = {english},
	doi = {10.1787/3d5c5d31-en},
	file = {Full text:C\:\\Users\\aless\\Zotero\\storage\\K84XL5L6\\OECD - 2023 - Government at a Glance 2023.pdf:application/pdf},
}

@article{boyne_evaluation_2002,
	title = {The Evaluation of Public Service Inspection: A Theoretical Framework},
	volume = {39},
	rights = {https://journals.sagepub.com/page/policies/text-and-data-mining-license},
	issn = {0042-0980, 1360-063X},
	url = {https://journals.sagepub.com/doi/10.1080/00420980220135563},
	doi = {10.1080/00420980220135563},
	shorttitle = {The Evaluation of Public Service Inspection},
	abstract = {Inspection is an important and growing element of the regulation of public services. We develop a theoretical framework for the evaluation of inspection and illustrate the practical relevance of this framework with reference to the new Best Value inspectorate for {UK} local government. The framework contains three elements that facilitate the effectiveness of an inspection system (a director, a detector and an effector), ve potential problems that impede its effectiveness (resistance, ritual compliance, regulatory capture, performance ambiguity and information gaps) and a pivotal mediating variable which is the expertise of inspectors. We combine these variables into a matrix that can be used as a checklist for the evaluation of inspection and we identify hypotheses concerning the circumstances that lead to successful inspection outcomes.},
	pages = {1197--1212},
	number = {7},
	journaltitle = {Urban Studies},
	shortjournal = {Urban Studies},
	author = {Boyne, George and Day, Patricia and Walker, Richard},
	urldate = {2025-03-27},
	date = {2002-06},
	langid = {english},
	file = {PDF:C\:\\Users\\aless\\Zotero\\storage\\5FS3J4R3\\Boyne et al. - 2002 - The Evaluation of Public Service Inspection A Theoretical Framework.pdf:application/pdf},
}

@article{qin_survey_2025,
	title = {A survey of multilingual large language models},
	volume = {6},
	issn = {2666-3899},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11783891/},
	doi = {10.1016/j.patter.2024.101118},
	abstract = {Multilingual large language models ({MLLMs}) leverage advanced large language models to process and respond to queries across multiple languages, achieving significant success in polyglot tasks. Despite these breakthroughs, a comprehensive survey summarizing existing approaches and recent developments remains absent. To this end, this paper presents a unified and thorough review of the field, highlighting recent progress and emerging trends in {MLLM} research. The contributions of this paper are as follows. (1) Extensive survey: to our knowledge, this is the pioneering thorough review of multilingual alignment in {MLLMs}. (2) Unified taxonomy: we provide a unified framework to summarize the current progress in {MLLMs}. (3) Emerging frontiers: key emerging frontiers are identified, alongside a discussion of associated challenges. (4) Abundant resources: we collect abundant open-source resources, including relevant papers, data corpora, and leaderboards. We hope our work can provide the community quick access and spur breakthrough research in {MLLMs}., The rapid advancement of large language models ({LLMs}) has significantly transformed natural language processing ({NLP}), enabling machines to understand and generate human-like text. However, most {LLMs} are predominantly English centric, limiting their applicability in our linguistically diverse world. With over 7,000 languages spoken globally, there is a pressing need for models that can comprehend and generate text across multiple languages. Multilingual large language models ({MLLMs}) address this gap by processing and producing content in various languages, thereby enhancing global communication and accessibility. This survey provides a comprehensive overview of {MLLMs}, introducing a systematic taxonomy based on alignment strategies to deepen understanding in this field. By highlighting emerging trends and challenges, this survey aims to guide future research and development, fostering the creation of more inclusive and effective language models that cater to the diverse linguistic landscape of our world., This survey explores the growing field of multilingual large language models ({MLLMs}), addressing the challenges of linguistic diversity in natural language processing. By introducing a systematic taxonomy based on alignment strategies, the authors provide a structured understanding of {MLLMs}' capabilities. The paper reviews current advancements, identifies emerging trends, and outlines challenges in building inclusive language models. This comprehensive overview offers critical insights to guide future research and development, promoting more equitable and effective multilingual communication in an increasingly interconnected world.},
	pages = {101118},
	number = {1},
	journaltitle = {Patterns},
	shortjournal = {Patterns (N Y)},
	author = {Qin, Libo and Chen, Qiguang and Zhou, Yuhang and Chen, Zhi and Li, Yinghui and Liao, Lizi and Li, Min and Che, Wanxiang and Yu, Philip S.},
	urldate = {2025-04-13},
	date = {2025-01-10},
	pmid = {39896256},
	pmcid = {PMC11783891},
	file = {Full Text PDF:C\:\\Users\\aless\\Zotero\\storage\\ARD97RGW\\Qin et al. - 2025 - A survey of multilingual large language models.pdf:application/pdf},
}

@article{binz_how_nodate,
	title = {How should the advancement of large language models affect the practice of science?},
	volume = {122},
	issn = {0027-8424},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11804466/},
	doi = {10.1073/pnas.2401227121},
	abstract = {Large language models ({LLMs}) are being increasingly incorporated into scientific workflows. However, we have yet to fully grasp the implications of this integration. How should the advancement of large language models affect the practice of science? For this opinion piece, we have invited four diverse groups of scientists to reflect on this query, sharing their perspectives and engaging in debate. Schulz et al. make the argument that working with {LLMs} is not fundamentally different from working with human collaborators, while Bender et al. argue that {LLMs} are often misused and overhyped, and that their limitations warrant a focus on more specialized, easily interpretable tools. Marelli et al. emphasize the importance of transparent attribution and responsible use of {LLMs}. Finally, Botvinick and Gershman advocate that humans should retain responsibility for determining the scientific roadmap. To facilitate the discussion, the four perspectives are complemented with a response from each group. By putting these different perspectives in conversation, we aim to bring attention to important considerations within the academic community regarding the adoption of {LLMs} and their impact on both current and future scientific practices.},
	pages = {e2401227121},
	number = {5},
	journaltitle = {Proceedings of the National Academy of Sciences of the United States of America},
	shortjournal = {Proc Natl Acad Sci U S A},
	author = {Binz, Marcel and Alaniz, Stephan and Roskies, Adina and Aczel, Balazs and Bergstrom, Carl T. and Allen, Colin and Schad, Daniel and Wulff, Dirk and West, Jevin D. and Zhang, Qiong and Shiffrin, Richard M. and Gershman, Samuel J. and Popov, Vencislav and Bender, Emily M. and Marelli, Marco and Botvinick, Matthew M. and Akata, Zeynep and Schulz, Eric},
	urldate = {2025-04-13},
	pmid = {39869798},
	pmcid = {PMC11804466},
	file = {PubMed Central Full Text PDF:C\:\\Users\\aless\\Zotero\\storage\\TDNC3687\\Binz et al. - How should the advancement of large language models affect the practice of science.pdf:application/pdf},
}

@article{papageorgiou_enhancing_2024,
	title = {Enhancing E-Government Services through State-of-the-Art, Modular, and Reproducible Architecture over Large Language Models},
	volume = {14},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/14/18/8259},
	doi = {10.3390/app14188259},
	abstract = {Integrating Large Language Models ({LLMs}) into e-government applications has the potential to improve public service delivery through advanced data processing and automation. This paper explores critical aspects of a modular and reproducible architecture based on Retrieval-Augmented Generation ({RAG}) for deploying {LLM}-based assistants within e-government systems. By examining current practices and challenges, we propose a framework ensuring that Artificial Intelligence ({AI}) systems are modular and reproducible, essential for maintaining scalability, transparency, and ethical standards. Our approach utilizing Haystack demonstrates a complete multi-agent Generative {AI} ({GAI}) virtual assistant that facilitates scalability and reproducibility by allowing individual components to be independently scaled. This research focuses on a comprehensive review of the existing literature and presents case study examples to demonstrate how such an architecture can enhance public service operations. This framework provides a valuable case study for researchers, policymakers, and practitioners interested in exploring the integration of advanced computational linguistics and {LLMs} into e-government services, although it could benefit from further empirical validation.},
	pages = {8259},
	number = {18},
	journaltitle = {Applied Sciences},
	author = {Papageorgiou, George and Sarlis, Vangelis and Maragoudakis, Manolis and Tjortjis, Christos},
	urldate = {2025-04-13},
	date = {2024-01},
	langid = {english},
	note = {Number: 18
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {{AI} Governance, e-government, generative artificial intelligence ({GAI}), large language models ({LLMs}), modularity, reproducibility, retrieval-augmented generation ({RAG})},
	file = {Full Text PDF:C\:\\Users\\aless\\Zotero\\storage\\V3SG7W27\\Papageorgiou et al. - 2024 - Enhancing E-Government Services through State-of-the-Art, Modular, and Reproducible Architecture ove.pdf:application/pdf},
}

@article{wang_large_2024,
	title = {Large language models and their application in government affairs},
	volume = {64},
	issn = {1000-0054},
	url = {https://www.sciopen.com/article/10.16511/j.cnki.qhdxxb.2023.26.042},
	doi = {10.16511/j.cnki.qhdxxb.2023.26.042},
	abstract = {{\textless}sec{\textgreater}{\textless}strong{\textgreater}Significance{\textless}/strong{\textgreater}{\textless}p{\textgreater}Since the turn of the 21st century, artificial intelligence ({AI}) has advanced considerably in many domains, including government affairs. Furthermore, the emergence of deep learning has taken the development of many {AI} fields, including natural language processing ({NLP}), to a new level. Language models ({LMs}) are key research directions of {NLP}. Referred to as statistical models, {LMs} were initially used to calculate the probability of a sentence; however, in recent years, there have been substantial developments in large language models ({LLMs}). Notably, {LLM} products, such as the generative pretrained transformer ({GPT}) series, have driven the rapid revolution of large language research. Domestic enterprises have also researched {LLMs}, for example, Huawei's Pangu and Baidu's enhanced language representation with informative entities ({ERNIE}) bot. These models have been widely used in language translation, abstract construction, named-entity recognition, text classification, and relationship extraction, among other applications, and in government affairs, finance, biomedicine, and other domains.{\textless}/p{\textgreater}{\textless}/sec{\textgreater}{\textless}sec{\textgreater}{\textless}strong{\textgreater}Progress{\textless}/strong{\textgreater}{\textless}p{\textgreater}In this study, we observe that improving the efficiency of governance has become one of the core tasks of the government in the era of big data. With the continuous accumulation of government data, traditional statistical models relying on expert experience and local features gradually suffer limitations during application. However, {LLMs}, which offer the advantages of high flexibility, strong representation ability, and effective results, can rapidly enhance the intelligence level of government services. First, we review the research progress on early {LMs}, such as statistical {LMs} and neural network {LMs}. Subsequently, we focus on the research progress on {LLMs}, namely the Transformers series, {GPT} series, and bidirectional encoder representations from transformers ({BERT}) series. Finally, we introduce the application of {LLMs} in government affairs, including government text classification, relationship extraction, public opinion risk identification, named-entity recognition, and government question answering. Moreover, we propose that research on {LLMs} for government affairs must focus on multimodality, correctly benefit from the trend of "model as a service, " focus on high data security, and clarify government responsibility boundaries. Additionally, a technical path for studying {LLMs} for government affairs has been proposed.{\textless}/p{\textgreater}{\textless}/sec{\textgreater}{\textless}sec{\textgreater}{\textless}strong{\textgreater}Conclusions and Prospects{\textless}/strong{\textgreater}{\textless}p{\textgreater}The application of {LLMs} in government affairs mainly focuses on small-scale models, lacking examples of application in large-scale models. Compared with smaller models, large models offer many advantages, including high efficiency, broader application scenarios, and more convenience. These advantages can be understood as follows. In terms of efficiency, large models are usually trained on a large amount of heterogeneous data, thus delivering better performance. In terms of application scenarios, large models gradually support multimodal data, resulting in more diverse application scenarios. In terms of convenience, we emphasize the "pretraining + fine-tuning" mode and the invocation method of interfaces, making {LLMs} more convenient for research and practical applications. This study also analyzes the issues suffered by {LLMs}, specifically from the technological and ethical perspectives, which have resulted in a panic to a certain extent. For example, {ChatGPT} has generated many controversies, including whether the generated files are novel, whether using {ChatGPT} will lead to plagiarism and ambiguity as to who are property rights owners for the generated files. Overall, it can be said that {LLMs} are in the stage of vigorous development. As the country promotes research on {AI} and its application in government affairs, {LLMs} will play an increasingly crucial role in the field.{\textless}/p{\textgreater}{\textless}/sec{\textgreater}},
	pages = {649--658},
	number = {4},
	journaltitle = {Journal of Tsinghua University (Science and Technology)},
	author = {Wang, Yun and Hu, Min and Ta, Na and Sun, Haitao and Guo, Yifeng and Zhou, Wuai and Guo, Yu and Zhang, Wanzhe and Feng, Jianhua},
	urldate = {2025-04-13},
	date = {2024-04-15},
	langid = {english},
	note = {Publisher: 清华大学出版社},
	file = {Full Text PDF:C\:\\Users\\aless\\Zotero\\storage\\UKGTBX8I\\Wang et al. - 2024 - Large language models and their application in government affairs.pdf:application/pdf},
}

@misc{rawal_responsible_2025,
	title = {Responsible Artificial Intelligence ({RAI}) in U.S. Federal Government : Principles, Policies, and Practices},
	url = {http://arxiv.org/abs/2502.03470},
	doi = {10.48550/arXiv.2502.03470},
	shorttitle = {Responsible Artificial Intelligence ({RAI}) in U.S. Federal Government},
	abstract = {Artificial intelligence ({AI}) and machine learning ({ML}) have made tremendous advancements in the past decades. From simple recommendation systems to more complex tumor identification systems, {AI}/{ML} systems have been utilized in a plethora of applications. This rapid growth of {AI}/{ML} and its proliferation in numerous private and public sector applications, while successful, has also opened new challenges and obstacles for regulators. With almost little to no human involvement required for some of the new decision-making {AI}/{ML} systems, there is now a pressing need to ensure the responsible use of these systems. Particularly in federal government use-cases, the use of {AI} technologies must be carefully governed by appropriate transparency and accountability mechanisms. This has given rise to new interdisciplinary fields of {AI} research such as {\textbackslash}textit\{Responsible {AI} ({RAI})\}. In this position paper we provide a brief overview of development in {RAI} and discuss some of the motivating principles commonly explored in the field. An overview of the current regulatory landscape relating to {AI} is also discussed with analysis of different Executive Orders, policies and frameworks. We then present examples of how federal agencies are aiming for the responsible use of {AI}, specifically we present use-case examples of different projects and research from the Census Bureau on implementing the responsible use of {AI}. We also provide a brief overview for a Responsible {AI} Assessment Toolkit currently under-development aimed at helping federal agencies operationalize {RAI} principles. Finally, a robust discussion on how different policies/regulations map to {RAI} principles, along with challenges and opportunities for regulation/governance of responsible {AI} within the federal government is presented.},
	number = {{arXiv}:2502.03470},
	publisher = {{arXiv}},
	author = {Rawal, Atul and Johnson, Katie and Mitchell, Curtis and Walton, Michael and Nwankwo, Diamond},
	urldate = {2025-04-13},
	date = {2025-01-12},
	eprinttype = {arxiv},
	eprint = {2502.03470 [cs]},
	keywords = {Computer Science - Computers and Society},
	file = {Preprint PDF:C\:\\Users\\aless\\Zotero\\storage\\G7E6S7F6\\Rawal et al. - 2025 - Responsible Artificial Intelligence (RAI) in U.S. Federal Government  Principles, Policies, and Pra.pdf:application/pdf;Snapshot:C\:\\Users\\aless\\Zotero\\storage\\ARR39UYL\\2502.html:text/html},
}

@misc{ojewale_towards_2025,
	title = {Towards {AI} Accountability Infrastructure: Gaps and Opportunities in {AI} Audit Tooling},
	url = {http://arxiv.org/abs/2402.17861},
	doi = {10.1145/3706598.3713301},
	shorttitle = {Towards {AI} Accountability Infrastructure},
	abstract = {Audits are critical mechanisms for identifying the risks and limitations of deployed artificial intelligence ({AI}) systems. However, the effective execution of {AI} audits remains incredibly difficult, and practitioners often need to make use of various tools to support their efforts. Drawing on interviews with 35 {AI} audit practitioners and a landscape analysis of 435 tools, we compare the current ecosystem of {AI} audit tooling to practitioner needs. While many tools are designed to help set standards and evaluate {AI} systems, they often fall short in supporting accountability. We outline challenges practitioners faced in their efforts to use {AI} audit tools and highlight areas for future tool development beyond evaluation -- from harms discovery to advocacy. We conclude that the available resources do not currently support the full scope of {AI} audit practitioners' needs and recommend that the field move beyond tools for just evaluation and towards more comprehensive infrastructure for {AI} accountability.},
	author = {Ojewale, Victor and Steed, Ryan and Vecchione, Briana and Birhane, Abeba and Raji, Inioluwa Deborah},
	urldate = {2025-04-14},
	date = {2025-02-27},
	eprinttype = {arxiv},
	eprint = {2402.17861 [cs]},
	keywords = {Computer Science - Computers and Society},
	file = {Full Text PDF:C\:\\Users\\aless\\Zotero\\storage\\ADQR2W2I\\Ojewale et al. - 2025 - Towards AI Accountability Infrastructure Gaps and Opportunities in AI Audit Tooling.pdf:application/pdf;Snapshot:C\:\\Users\\aless\\Zotero\\storage\\UYHRDYC6\\2402.html:text/html},
}

@misc{bell_think_2022,
	title = {Think About the Stakeholders First! Towards an Algorithmic Transparency Playbook for Regulatory Compliance},
	url = {http://arxiv.org/abs/2207.01482},
	doi = {10.48550/arXiv.2207.01482},
	abstract = {Increasingly, laws are being proposed and passed by governments around the world to regulate Artificial Intelligence ({AI}) systems implemented into the public and private sectors. Many of these regulations address the transparency of {AI} systems, and related citizen-aware issues like allowing individuals to have the right to an explanation about how an {AI} system makes a decision that impacts them. Yet, almost all {AI} governance documents to date have a significant drawback: they have focused on what to do (or what not to do) with respect to making {AI} systems transparent, but have left the brunt of the work to technologists to figure out how to build transparent systems. We fill this gap by proposing a novel stakeholder-first approach that assists technologists in designing transparent, regulatory compliant systems. We also describe a real-world case-study that illustrates how this approach can be used in practice.},
	number = {{arXiv}:2207.01482},
	publisher = {{arXiv}},
	author = {Bell, Andrew and Nov, Oded and Stoyanovich, Julia},
	urldate = {2025-04-14},
	date = {2022-06-10},
	eprinttype = {arxiv},
	eprint = {2207.01482 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction},
	file = {Full Text PDF:C\:\\Users\\aless\\Zotero\\storage\\ILPXQJYA\\Bell et al. - 2022 - Think About the Stakeholders First! Towards an Algorithmic Transparency Playbook for Regulatory Comp.pdf:application/pdf;Snapshot:C\:\\Users\\aless\\Zotero\\storage\\C5NC2HLC\\2207.html:text/html},
}

@misc{coglianese_regulating_2017,
	location = {Rochester, {NY}},
	title = {Regulating by Robot: Administrative Decision Making in the Machine-Learning Era},
	url = {https://papers.ssrn.com/abstract=2928293},
	shorttitle = {Regulating by Robot},
	abstract = {Machine-learning algorithms are transforming large segments of the economy as they fuel innovation in search engines, self-driving cars, product marketing, and medical imaging, among many other technologies. As machine learning’s use expands across all facets of society, anxiety has emerged about the intrusion of algorithmic machines into facets of life previously dependent on human judgment. Alarm bells sounding over the diffusion of artificial intelligence throughout the private sector only portend greater anxiety about digital robots replacing humans in the governmental sphere. A few administrative agencies have already begun to adopt this technology, while others have clear potential in the near term to use algorithms to shape official decisions over both rulemaking and adjudication. It is no longer fanciful to envision a future in which government agencies could effectively make law by robot, a prospect that understandably conjures up dystopian images of individuals surrendering their liberty to the control of computerized overlords. Should society be alarmed by governmental use of machine-learning applications? We examine this question by considering whether the use of robotic decision tools by government agencies can pass muster under core, time-honored doctrines of administrative and constitutional law. At first glance, the idea of algorithmic regulation might appear to offend one or more traditional doctrines, such as the nondelegation doctrine, procedural due process, equal protection, or principles of reason-giving and transparency. We conclude, however, that when machine-learning technology is properly understood, its use by government agencies can comfortably fit within these conventional legal parameters. We recognize, of course, that the legality of regulation by robot is only one criterion by which its use should be assessed. Agencies should not apply algorithms cavalierly, even if doing so might not run afoul of the law; in some cases, safeguards may be needed for machine learning to satisfy broader, good-governance aspirations. Yet, in contrast with the emerging alarmism, we resist any categorical dismissal of a future administrative state in which algorithmic automation guides, and even at times makes, key decisions. Instead, we urge that governmental reliance on machine learning should be approached with measured optimism about the potential benefits such technology can offer society by making government smarter and its decisions more efficient and just.},
	number = {2928293},
	publisher = {Social Science Research Network},
	author = {Coglianese, Cary and Lehr, David},
	urldate = {2025-04-14},
	date = {2017-06-01},
	langid = {english},
	keywords = {e-government, adjudicating by algorithm, agent-based models, {AI}, artificial intelligence, automation, autonomous systems, Constitutional \& administrative law, digital government, information technology, multi-agent systems, rulemaking by robot, statistical learning},
	file = {Full Text PDF:C\:\\Users\\aless\\Zotero\\storage\\F7TE8Y7S\\Coglianese e Lehr - 2017 - Regulating by Robot Administrative Decision Making in the Machine-Learning Era.pdf:application/pdf},
}

@misc{landau_challenging_2024,
	title = {Challenging the Machine: Contestability in Government {AI} Systems},
	url = {http://arxiv.org/abs/2406.10430},
	doi = {10.48550/arXiv.2406.10430},
	shorttitle = {Challenging the Machine},
	abstract = {In an October 2023 executive order ({EO}), President Biden issued a detailed but largely aspirational road map for the safe and responsible development and use of artificial intelligence ({AI}). The challenge for the January 24-25, 2024 workshop was to transform those aspirations regarding one specific but crucial issue -- the ability of individuals to challenge government decisions made about themselves -- into actionable guidance enabling agencies to develop, procure, and use genuinely contestable advanced automated decision-making systems. While the Administration has taken important steps since the October 2023 {EO}, the insights garnered from our workshop remain highly relevant, as the requirements for contestability of advanced decision-making systems are not yet fully defined or implemented. The workshop brought together technologists, members of government agencies and civil society organizations, litigators, and researchers in an intensive two-day meeting that examined the challenges that users, developers, and agencies faced in enabling contestability in light of advanced automated decision-making systems. To ensure a free and open flow of discussion, the meeting was held under a modified version of the Chatham House rule. Participants were free to use any information or details that they learned, but they may not attribute any remarks made at the meeting by the identity or the affiliation of the speaker. Thus, the workshop summary that follows anonymizes speakers and their affiliation. Where an identification of an agency, company, or organization is made, it is done from a public, identified resource and does not necessarily reflect statements made by participants at the workshop. This document is a report of that workshop, along with recommendations and explanatory material.},
	number = {{arXiv}:2406.10430},
	publisher = {{arXiv}},
	author = {Landau, Susan and Dempsey, James X. and Kamar, Ece and Bellovin, Steven M. and Pool, Robert},
	urldate = {2025-04-14},
	date = {2024-06-14},
	eprinttype = {arxiv},
	eprint = {2406.10430 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	file = {Full Text PDF:C\:\\Users\\aless\\Zotero\\storage\\YGTURHPP\\Landau et al. - 2024 - Challenging the Machine Contestability in Government AI Systems.pdf:application/pdf;Snapshot:C\:\\Users\\aless\\Zotero\\storage\\DSWLCYD9\\2406.html:text/html},
}

@report{schwartz_towards_2022,
	location = {Gaithersburg, {MD}},
	title = {Towards a standard for identifying and managing bias in artificial intelligence},
	url = {https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1270.pdf},
	abstract = {As individuals and communities interact in and with an environment that is increasingly virtual they are often vulnerable to the commodification of their digital exhaust. Concepts and behavior that are ambiguous in nature are captured in this environment, quantified, and used to categorize, sort, recommend, or make decisions about people's lives. While many organizations seek to utilize this information in a responsible manner, biases remain endemic across technology processes and can lead to harmful impacts regardless of intent. These harmful outcomes, even if inadvertent, create significant challenges for cultivating public trust in artificial intelligence ({AI}). {SP} 1270 is a {NIST} Artificial Intelligence publication and should be read in conjunction with all publications in the {NIST} {AI} Series, which was established in January 2023.},
	pages = {NIST SP 1270},
	number = {{NIST} {SP} 1270},
	institution = {National Institute of Standards and Technology (U.S.)},
	author = {Schwartz, Reva and Vassilev, Apostol and Greene, Kristen and Perine, Lori and Burt, Andrew and Hall, Patrick},
	urldate = {2025-04-14},
	date = {2022-03-15},
	langid = {english},
	doi = {10.6028/NIST.SP.1270},
	file = {PDF:C\:\\Users\\aless\\Zotero\\storage\\SQT2VJH7\\Schwartz et al. - 2022 - Towards a standard for identifying and managing bias in artificial intelligence.pdf:application/pdf},
}

@misc{zhao_survey_2025,
	title = {A Survey of Large Language Models},
	url = {http://arxiv.org/abs/2303.18223},
	doi = {10.48550/arXiv.2303.18223},
	abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable {AI} algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models ({PLMs}) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various {NLP} tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models ({LLM}) for the {PLMs} of significant size. Recently, the research on {LLMs} has been largely advanced by both academia and industry, and a remarkable progress is the launch of {ChatGPT}, which has attracted widespread attention from society. The technical evolution of {LLMs} has been making an important impact on the entire {AI} community, which would revolutionize the way how we develop and use {AI} algorithms. In this survey, we review the recent advances of {LLMs} by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of {LLMs}, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing {LLMs} and discuss the remaining issues for future directions.},
	number = {{arXiv}:2303.18223},
	publisher = {{arXiv}},
	author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
	urldate = {2025-04-14},
	date = {2025-03-11},
	eprinttype = {arxiv},
	eprint = {2303.18223 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\aless\\Zotero\\storage\\GGW4WI6X\\Zhao et al. - 2025 - A Survey of Large Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\aless\\Zotero\\storage\\KAF3GHNH\\2303.html:text/html},
}

@article{weerts_generative_2025,
	title = {Generative {AI} in public administration in light of the regulatory awakening in the {US} and {EU}},
	volume = {1},
	issn = {3033-3733},
	url = {https://www.cambridge.org/core/journals/cambridge-forum-on-ai-law-and-governance/article/generative-ai-in-public-administration-in-light-of-the-regulatory-awakening-in-the-us-and-eu/854F0A4A5DB37A2A15DF3835BFCA9484},
	doi = {10.1017/cfl.2024.10},
	abstract = {This paper explores the regulatory awakening regarding generative {AI} ({GenAI}) in the United States and European Union ({EU}) institutions with the release of {ChatGPT}. Based on a thematic analysis of regulatory documents, it investigates how governments have approached the deployment and use of this emerging technology within their classic government activities. The analysis shows several layers of regulatory approaches, ranging from command-and-control to an experimental approach, combined with risk- and management-based approaches. It also reveals different perspectives. The {EU} institutions have notably adopted more restrictive guidelines on the use of publicly available Large Language Models ({LLMs}) - a type o {GenAI} that is trained on vast amounts of text data to understand, generate, and respond in human-like language. This approach reflects greater caution about data security and confidentiality and the risks of foreign interference. However, the American and {EU} documents share a common concern about the risk of reinforcing discrimination and the protection of human rights. Interestingly, considering the administrative environment, neither the administrative activities in which {GenAI} may be used nor the key legal principles embedded by the rule of law are explicitly used for guiding administration in their development and use of {GenAI}. In this context, the paper calls for future research that could help contribute to the renewal of administrative law theory in the context of the digital transformation of public administration.},
	pages = {e3},
	journaltitle = {Cambridge Forum on {AI}: Law and Governance},
	author = {Weerts, Sophie},
	urldate = {2025-04-14},
	date = {2025-01},
	langid = {english},
	keywords = {European Union, generative {AI}, public administration, regulation, regulatory approaches, {US}},
	file = {Full Text PDF:C\:\\Users\\aless\\Zotero\\storage\\C58D8SNX\\Weerts - 2025 - Generative AI in public administration in light of the regulatory awakening in the US and EU.pdf:application/pdf},
}

@article{liao_ai_2024,
	title = {{AI} Transparency in the Age of {LLMs}: A Human-Centered Research Roadmap},
	issn = {2644-2353, 2688-8513},
	url = {https://hdsr.mitpress.mit.edu/pub/aelql9qy/release/2},
	doi = {10.1162/99608f92.8036d03b},
	shorttitle = {{AI} Transparency in the Age of {LLMs}},
	abstract = {The rise of powerful large language models ({LLMs}) brings about tremendous opportunities for innovation but also looming risks for individuals and society at large. We have reached a pivotal moment for ensuring that {LLMs} and {LLM}-infused applications are developed and deployed responsibly. It is paramount to pursue new approaches to provide transparency—a central pillar of responsible artificial intelligence ({AI})—for {LLMs}, and years of research at the intersection of {AI} and human–computer interaction ({HCI}) highlight that we must do so with a human-centered perspective: Transparency is fundamentally about supporting appropriate human understanding, and this understanding is sought by different stakeholders with different goals in different contexts. In this new era of {LLMs}, we must develop and design approaches to transparency by considering the needs of stakeholders in the emerging {LLM} ecosystem, the novel types of {LLM}-infused applications being built, and the new usage patterns and challenges around {LLMs}, all while building on lessons learned about how people process, interact with, and make use of information. We reflect on the unique challenges that arise in providing transparency for {LLMs}, along with lessons learned from {HCI} and responsible {AI} research that has taken a human-centered perspective on {AI} transparency. We then lay out four common approaches that the community has taken to achieve transparency—model reporting, publishing evaluation results, providing explanations, and communicating uncertainty—and call out open questions around how these approaches may or may not be applied to {LLMs}. We hope this provides a starting point for discussion and a useful roadmap for future research.},
	issue = {Special Issue 5},
	journaltitle = {Harvard Data Science Review},
	author = {Liao, Q. Vera and Vaughan, Jennifer Wortman},
	urldate = {2025-04-14},
	date = {2024-05-31},
	langid = {english},
	note = {Publisher: The {MIT} Press},
	file = {Full Text PDF:C\:\\Users\\aless\\Zotero\\storage\\W6ARVXTA\\Liao e Vaughan - 2024 - AI Transparency in the Age of LLMs A Human-Centered Research Roadmap.pdf:application/pdf},
}

@article{ibrahim_beyond_2024,
	title = {Beyond static {AI} evaluations: advancing human interaction evaluations for {LLM} harms and risks},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2405.10632},
	doi = {10.48550/ARXIV.2405.10632},
	shorttitle = {Beyond static {AI} evaluations},
	abstract = {Model evaluations are central to understanding the safety, risks, and societal impacts of {AI} systems. While most real-world {AI} applications involve human-{AI} interaction, most current evaluations (e.g., common benchmarks) of {AI} models do not. Instead, they incorporate human factors in limited ways, assessing the safety of models in isolation, thereby falling short of capturing the complexity of human-model interactions. In this paper, we discuss and operationalize a definition of an emerging category of evaluations -- "human interaction evaluations" ({HIEs}) -- which focus on the assessment of human-model interactions or the process and the outcomes of humans using models. First, we argue that {HIEs} can be used to increase the validity of safety evaluations, assess direct human impact and interaction-specific harms, and guide future assessments of models' societal impact. Second, we propose a safety-focused {HIE} design framework -- containing a human-{LLM} interaction taxonomy -- with three stages: (1) identifying the risk or harm area, (2) characterizing the use context, and (3) choosing the evaluation parameters. Third, we apply our framework to two potential evaluations for overreliance and persuasion risks. Finally, we conclude with tangible recommendations for addressing concerns over costs, replicability, and unrepresentativeness of {HIEs}.},
	author = {Ibrahim, Lujain and Huang, Saffron and Ahmad, Lama and Anderljung, Markus},
	urldate = {2025-04-14},
	date = {2024},
	note = {Publisher: {arXiv}
Version Number: 5},
	keywords = {{FOS}: Computer and information sciences, Artificial Intelligence (cs.{AI}), Computers and Society (cs.{CY}), Human-Computer Interaction (cs.{HC})},
	file = {Full Text PDF:C\:\\Users\\aless\\Zotero\\storage\\ZR39KANV\\Ibrahim et al. - 2024 - Beyond static AI evaluations advancing human interaction evaluations for LLM harms and risks.pdf:application/pdf},
}

@article{kong_large_2024,
	title = {Large Language Models for Financial and Investment Management: Models, Opportunities, and Challenges},
	volume = {51},
	issn = {0095-4918, 2168-8656},
	url = {http://pm-research.com/lookup/doi/10.3905/jpm.2024.1.646},
	doi = {10.3905/jpm.2024.1.646},
	shorttitle = {Large Language Models for Financial and Investment Management},
	pages = {211--231},
	number = {2},
	journaltitle = {The Journal of Portfolio Management},
	shortjournal = {{JPM}},
	author = {Kong, Yaxuan and Nie, Yuqi and Dong, Xiaowen and Mulvey, John M. and Poor, H. Vincent and Wen, Qingsong and Zohren, Stefan},
	urldate = {2025-04-14},
	date = {2024-11-30},
	langid = {english},
	file = {PDF:C\:\\Users\\aless\\Zotero\\storage\\K4T2EF2E\\Kong et al. - 2024 - Large Language Models for Financial and Investment Management Models, Opportunities, and Challenges.pdf:application/pdf},
}

@inproceedings{cheong_i_2024,
	location = {Rio de Janeiro Brazil},
	title = {(A)I Am Not a Lawyer, But...: Engaging Legal Experts towards Responsible {LLM} Policies for Legal Advice},
	isbn = {979-8-4007-0450-5},
	url = {https://dl.acm.org/doi/10.1145/3630106.3659048},
	doi = {10.1145/3630106.3659048},
	shorttitle = {(A)I Am Not a Lawyer, But...},
	eventtitle = {{FAccT} '24: The 2024 {ACM} Conference on Fairness, Accountability, and Transparency},
	pages = {2454--2469},
	booktitle = {The 2024 {ACM} Conference on Fairness, Accountability, and Transparency},
	publisher = {{ACM}},
	author = {Cheong, Inyoung and Xia, King and Feng, K. J. Kevin and Chen, Quan Ze and Zhang, Amy X.},
	urldate = {2025-04-14},
	date = {2024-06-03},
	langid = {english},
	file = {Versione inviata:C\:\\Users\\aless\\Zotero\\storage\\QXENIWXW\\Cheong et al. - 2024 - (A)I Am Not a Lawyer, But... Engaging Legal Experts towards Responsible LLM Policies for Legal Advi.pdf:application/pdf},
}

@article{clavell_checklist_nodate,
	title = {Checklist for {AI} Auditing},
	author = {Clavell, Dr Gemma {GALDON}},
	langid = {english},
	file = {PDF:C\:\\Users\\aless\\Zotero\\storage\\KDQKTNQK\\Clavell - Checklist for AI Auditing.pdf:application/pdf},
}

@article{savelka_unreasonable_2023,
	title = {The unreasonable effectiveness of large language models in zero-shot semantic annotation of legal texts},
	volume = {6},
	issn = {2624-8212},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10690809/},
	doi = {10.3389/frai.2023.1279794},
	abstract = {The emergence of {ChatGPT} has sensitized the general public, including the legal profession, to large language models' ({LLMs}) potential uses (e.g., document drafting, question answering, and summarization). Although recent studies have shown how well the technology performs in diverse semantic annotation tasks focused on legal texts, an influx of newer, more capable ({GPT}-4) or cost-effective ({GPT}-3.5-turbo) models requires another analysis. This paper addresses recent developments in the ability of {LLMs} to semantically annotate legal texts in zero-shot learning settings. Given the transition to mature generative {AI} systems, we examine the performance of {GPT}-4 and {GPT}-3.5-turbo(-16k), comparing it to the previous generation of {GPT} models, on three legal text annotation tasks involving diverse documents such as adjudicatory opinions, contractual clauses, or statutory provisions. We also compare the models' performance and cost to better understand the trade-offs. We found that the {GPT}-4 model clearly outperforms the {GPT}-3.5 models on two of the three tasks. The cost-effective {GPT}-3.5-turbo matches the performance of the 20× more expensive text-davinci-003 model. While one can annotate multiple data points within a single prompt, the performance degrades as the size of the batch increases. This work provides valuable information relevant for many practical applications (e.g., in contract review) and research projects (e.g., in empirical legal studies). Legal scholars and practicing lawyers alike can leverage these findings to guide their decisions in integrating {LLMs} in a wide range of workflows involving semantic annotation of legal texts.},
	pages = {1279794},
	journaltitle = {Frontiers in Artificial Intelligence},
	shortjournal = {Front Artif Intell},
	author = {Savelka, Jaromir and Ashley, Kevin D.},
	urldate = {2025-04-15},
	date = {2023-11-17},
	pmid = {38045764},
	pmcid = {PMC10690809},
	file = {PubMed Central Full Text PDF:C\:\\Users\\aless\\Zotero\\storage\\LI43MITM\\Savelka e Ashley - 2023 - The unreasonable effectiveness of large language models in zero-shot semantic annotation of legal te.pdf:application/pdf},
}

@misc{yu_legal_2022,
	title = {Legal Prompting: Teaching a Language Model to Think Like a Lawyer},
	url = {http://arxiv.org/abs/2212.01326},
	doi = {10.48550/arXiv.2212.01326},
	shorttitle = {Legal Prompting},
	abstract = {Large language models that are capable of zero or few-shot prompting approaches have given rise to the new research area of prompt engineering. Recent advances showed that for example Chain-of-Thought ({CoT}) prompts can improve arithmetic or common sense tasks significantly. We explore how such approaches fare with legal reasoning tasks and take the {COLIEE} entailment task based on the Japanese Bar exam for testing zero-shot/few-shot and fine-tuning approaches. Our findings show that while {CoT} prompting and fine-tuning with explanations approaches show improvements, the best results are produced by prompts that are derived from specific legal reasoning techniques such as {IRAC} (Issue, Rule, Application, Conclusion). Based on our experiments we improve the 2021 best result from 0.7037 accuracy to 0.8148 accuracy and beat the 2022 best system of 0.6789 accuracy with an accuracy of 0.7431.},
	number = {{arXiv}:2212.01326},
	publisher = {{arXiv}},
	author = {Yu, Fangyi and Quartey, Lee and Schilder, Frank},
	urldate = {2025-04-15},
	date = {2022-12-08},
	eprinttype = {arxiv},
	eprint = {2212.01326 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\aless\\Zotero\\storage\\2IZCZLKL\\Yu et al. - 2022 - Legal Prompting Teaching a Language Model to Think Like a Lawyer.pdf:application/pdf;Snapshot:C\:\\Users\\aless\\Zotero\\storage\\NCZM3HYZ\\2212.html:text/html},
}

@article{anthis_impossibility_2024,
	title = {The Impossibility of Fair {LLMs}},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2406.03198},
	doi = {10.48550/ARXIV.2406.03198},
	abstract = {The need for fair {AI} is increasingly clear in the era of general-purpose systems such as {ChatGPT}, Gemini, and other large language models ({LLMs}). However, the increasing complexity of human-{AI} interaction and its social impacts have raised questions of how fairness standards could be applied. Here, we review the technical frameworks that machine learning researchers have used to evaluate fairness, such as group fairness and fair representations, and find that their application to {LLMs} faces inherent limitations. We show that each framework either does not logically extend to {LLMs} or presents a notion of fairness that is intractable for {LLMs}, primarily due to the multitudes of populations affected, sensitive attributes, and use cases. To address these challenges, we develop guidelines for the more realistic goal of achieving fairness in particular use cases: the criticality of context, the responsibility of {LLM} developers, and the need for stakeholder participation in an iterative process of design and evaluation. Moreover, it may eventually be possible and even necessary to use the general-purpose capabilities of {AI} systems to address fairness challenges as a form of scalable {AI}-assisted alignment.},
	author = {Anthis, Jacy and Lum, Kristian and Ekstrand, Michael and Feller, Avi and D'Amour, Alexander and Tan, Chenhao},
	urldate = {2025-04-15},
	date = {2024},
	note = {Publisher: {arXiv}
Version Number: 1},
	keywords = {Applications (stat.{AP}), Computation and Language (cs.{CL}), {FOS}: Computer and information sciences, Human-Computer Interaction (cs.{HC}), Machine Learning (cs.{LG}), Machine Learning (stat.{ML})},
	file = {Full Text PDF:C\:\\Users\\aless\\Zotero\\storage\\XKTEGIFY\\Anthis et al. - 2024 - The Impossibility of Fair LLMs.pdf:application/pdf},
}

@misc{zou_docbench_2024,
	title = {{DOCBENCH}: A Benchmark for Evaluating {LLM}-based Document Reading Systems},
	url = {http://arxiv.org/abs/2407.10701},
	doi = {10.48550/arXiv.2407.10701},
	shorttitle = {{DOCBENCH}},
	abstract = {Recently, there has been a growing interest among large language model ({LLM}) developers in {LLM}-based document reading systems, which enable users to upload their own documents and pose questions related to the document contents, going beyond simple reading comprehension tasks. Consequently, these systems have been carefully designed to tackle challenges such as file parsing, metadata extraction, multi-modal information understanding and long-context reading. However, no current benchmark exists to evaluate their performance in such scenarios, where a raw file and questions are provided as input, and a corresponding response is expected as output. In this paper, we introduce {DocBench}, a new benchmark designed to evaluate {LLM}-based document reading systems. Our benchmark involves a meticulously crafted process, including the recruitment of human annotators and the generation of synthetic questions. It includes 229 real documents and 1,102 questions, spanning across five different domains and four major types of questions. We evaluate both proprietary {LLM}-based systems accessible via web interfaces or {APIs}, and a parse-then-read pipeline employing open-source {LLMs}. Our evaluations reveal noticeable gaps between existing {LLM}-based document reading systems and human performance, underscoring the challenges of developing proficient systems. To summarize, {DocBench} aims to establish a standardized benchmark for evaluating {LLM}-based document reading systems under diverse real-world scenarios, thereby guiding future advancements in this research area.},
	number = {{arXiv}:2407.10701},
	publisher = {{arXiv}},
	author = {Zou, Anni and Yu, Wenhao and Zhang, Hongming and Ma, Kaixin and Cai, Deng and Zhang, Zhuosheng and Zhao, Hai and Yu, Dong},
	urldate = {2025-04-15},
	date = {2024-07-15},
	eprinttype = {arxiv},
	eprint = {2407.10701 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\aless\\Zotero\\storage\\9N9T5536\\Zou et al. - 2024 - DOCBENCH A Benchmark for Evaluating LLM-based Document Reading Systems.pdf:application/pdf;Snapshot:C\:\\Users\\aless\\Zotero\\storage\\YYV6SBRI\\2407.html:text/html},
}

@article{ferraris_architecture_2025,
	title = {The architecture of language: Understanding the mechanics behind {LLMs}},
	volume = {1},
	issn = {3033-3733},
	url = {https://www.cambridge.org/core/journals/cambridge-forum-on-ai-law-and-governance/article/architecture-of-language-understanding-the-mechanics-behind-llms/E3DDEFB9C04883733380E04331D6F782},
	doi = {10.1017/cfl.2024.16},
	shorttitle = {The architecture of language},
	abstract = {Large language models ({LLMs}) have significantly advanced artificial intelligence ({AI}) and natural language processing ({NLP}) by excelling in tasks like text generation, machine translation, question answering and sentiment analysis, often rivaling human performance. This paper reviews {LLMs}’ foundations, advancements and applications, beginning with the transformative transformer architecture, which improved on earlier models like recurrent neural networks and convolutional neural networks through self-attention mechanisms that capture long-range dependencies and contextual relationships. Key innovations such as masked language modeling and causal language modeling underpin leading models like Bidirectional encoder representations from transformers ({BERT}) and the Generative Pre-trained Transformer ({GPT}) series. The paper highlights scaling laws, model size increases and advanced training techniques that have driven {LLMs}’ growth. It also explores methodologies to enhance their precision and adaptability, including parameter-efficient fine-tuning and prompt engineering. Challenges like high computational demands, biases and hallucinations are addressed, with solutions such as retrieval-augmented generation to improve factual accuracy. By discussing {LLMs}’ strengths, limitations and transformative potential, this paper provides researchers, practitioners and students with a comprehensive understanding. It underscores the importance of ongoing research to improve efficiency, manage ethical concerns and shape the future of {AI} and language technologies.},
	pages = {e11},
	journaltitle = {Cambridge Forum on {AI}: Law and Governance},
	author = {Ferraris, Andrea Filippo and Audrito, Davide and Caro, Luigi Di and Poncibò, Cristina},
	urldate = {2025-04-15},
	date = {2025-01},
	langid = {english},
	keywords = {artificial intelligence ({AI}), large language models ({LLMs}), natural language processing ({NLP})},
	file = {Full Text PDF:C\:\\Users\\aless\\Zotero\\storage\\GV4LC5H9\\Ferraris et al. - 2025 - The architecture of language Understanding the mechanics behind LLMs.pdf:application/pdf},
}

@article{stevens_theory_2023,
	title = {Theory of Trust and Acceptance of Artificial Intelligence Technology ({TrAAIT}): An Instrument to Assess Clinician Trust and Acceptance of Artificial Intelligence},
	volume = {148},
	issn = {1532-0464},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10815802/},
	doi = {10.1016/j.jbi.2023.104550},
	shorttitle = {Theory of Trust and Acceptance of Artificial Intelligence Technology ({TrAAIT})},
	pages = {104550},
	journaltitle = {Journal of biomedical informatics},
	shortjournal = {J Biomed Inform},
	author = {Stevens, Alexander F and Stetson, Pete},
	urldate = {2025-04-15},
	date = {2023-12},
	pmid = {37981107},
	pmcid = {PMC10815802},
	file = {PubMed Central Full Text PDF:C\:\\Users\\aless\\Zotero\\storage\\YDGGAHTR\\Stevens e Stetson - 2023 - Theory of Trust and Acceptance of Artificial Intelligence Technology (TrAAIT) An Instrument to Asse.pdf:application/pdf},
}

@article{hickok_public_2022,
	title = {Public procurement of artificial intelligence systems: new risks and future proofing},
	issn = {0951-5666},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9526785/},
	doi = {10.1007/s00146-022-01572-2},
	shorttitle = {Public procurement of artificial intelligence systems},
	abstract = {Public entities around the world are increasingly deploying artificial intelligence ({AI}) and algorithmic decision-making systems to provide public services or to use their enforcement powers. The rationale for the public sector to use these systems is similar to private sector: increase efficiency and speed of transactions and lower the costs. However, public entities are first and foremost established to meet the needs of the members of society and protect the safety, fundamental rights, and wellbeing of those they serve. Currently {AI} systems are deployed by the public sector at various administrative levels without robust due diligence, monitoring, or transparency. This paper critically maps out the challenges in procurement of {AI} systems by public entities and the long-term implications necessitating {AI}-specific procurement guidelines and processes. This dual-prong exploration includes the new complexities and risks introduced by {AI} systems, and the institutional capabilities impacting the decision-making process. {AI}-specific public procurement guidelines are urgently needed to protect fundamental rights and due process.},
	pages = {1--15},
	journaltitle = {Ai \& Society},
	shortjournal = {{AI} Soc},
	author = {Hickok, Merve},
	urldate = {2025-04-15},
	date = {2022-10-02},
	pmid = {36212228},
	pmcid = {PMC9526785},
	file = {PubMed Central Full Text PDF:C\:\\Users\\aless\\Zotero\\storage\\XLH6QQP4\\Hickok - 2022 - Public procurement of artificial intelligence systems new risks and future proofing.pdf:application/pdf},
}

@article{straub_artificial_2023,
	title = {Artificial intelligence in government: Concepts, standards, and a unified framework},
	volume = {40},
	issn = {0740624X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0740624X23000813},
	doi = {10.1016/j.giq.2023.101881},
	shorttitle = {Artificial intelligence in government},
	abstract = {Recent advances in artificial intelligence ({AI}) and machine learning ({ML}) hold the promise of improving government. Given the advanced capabilities of {AI} applications, it is critical that these are embedded using standard operational procedures, clear epistemic criteria, and behave in alignment with the normative expectations of society. Scholars in multiple domains have subsequently begun to conceptualize the different forms that {AI} systems may take, highlighting both their potential benefits and pitfalls. However, the literature remains fragmented, with researchers in social science disciplines like public administration and political science, and the fast-moving fields of {AI}, {ML}, and robotics, all developing concepts in relative isolation. Although there are calls to formalize the emerging study of {AI} in government, a balanced account that captures the full breadth of theoretical perspectives needed to understand the consequences of embedding {AI} into a public sector context is lacking. Here, we unify efforts across social and technical disciplines by using concept mapping to identify 107 different terms used in the multidisciplinary study of {AI}. We inductively sort these into three distinct semantic groups, which we label the (a) operational, (b) epistemic, and (c) normative domains. We then build on the results of this mapping exercise by proposing three new multifaceted concepts to study {AI}-based systems for government ({AI}-{GOV}) in an integrated, forward-looking way, which we call (1) operational fitness, (2) epistemic completeness, and (3) normative salience. Finally, we put these concepts to work by using them as dimensions in a conceptual typology of {AI}-{GOV} and connecting each with emerging {AI} technical measurement standards to encourage operationalization, foster cross-disciplinary dialogue, and stimulate debate among those aiming to reshape public administration with {AI}.},
	pages = {101881},
	number = {4},
	journaltitle = {Government Information Quarterly},
	shortjournal = {Government Information Quarterly},
	author = {Straub, Vincent J. and Morgan, Deborah and Bright, Jonathan and Margetts, Helen},
	urldate = {2025-04-15},
	date = {2023-10},
	langid = {english},
	file = {PDF:C\:\\Users\\aless\\Zotero\\storage\\YHLVBDYP\\Straub et al. - 2023 - Artificial intelligence in government Concepts, standards, and a unified framework.pdf:application/pdf},
}

@article{noauthor_pdf_2025,
	title = {({PDF}) Exploring {LLMs} Applications in Law: A Literature Review on Current Legal {NLP} Approaches},
	url = {https://www.researchgate.net/publication/388345767_Exploring_LLMs_Applications_in_Law_A_Literature_Review_on_Current_Legal_NLP_Approaches},
	doi = {10.1109/ACCESS.2025.3533217},
	shorttitle = {({PDF}) Exploring {LLMs} Applications in Law},
	abstract = {{PDF} {\textbar} Artificial intelligence is reshaping the legal landscape, with software tools now impacting various aspects of legal work. The intersection of... {\textbar} Find, read and cite all the research you need on {ResearchGate}},
	journaltitle = {{ResearchGate}},
	urldate = {2025-04-15},
	date = {2025-03-23},
	langid = {english},
	file = {Full Text PDF:C\:\\Users\\aless\\Zotero\\storage\\MABRNBYY\\2025 - (PDF) Exploring LLMs Applications in Law A Literature Review on Current Legal NLP Approaches.pdf:application/pdf;Snapshot:C\:\\Users\\aless\\Zotero\\storage\\PLVJJQSK\\388345767_Exploring_LLMs_Applications_in_Law_A_Literature_Review_on_Current_Legal_NLP_Approache.html:text/html},
}

@article{padiu_what_2024,
	title = {To What Extent Have {LLMs} Reshaped the Legal Domain So Far? A Scoping Literature Review},
	volume = {15},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2078-2489},
	url = {https://www.mdpi.com/2078-2489/15/11/662},
	doi = {10.3390/info15110662},
	shorttitle = {To What Extent Have {LLMs} Reshaped the Legal Domain So Far?},
	abstract = {Understanding and explaining legal systems is very challenging due to their complex structure, specialized terminology, and multiple interpretations. Legal {AI} models are currently undergoing drastic advancements due to the development of Large Language Models ({LLMs}) that have achieved state-of-the-art performance on a wide range of tasks and are currently undergoing very rapid iterations. As an emerging field, the application of {LLMs} in the legal field is still in its early stages, with multiple challenges that need to be addressed. Our objective is to provide a comprehensive survey of legal {LLMs}, not only reviewing the models themselves but also analyzing their applications within the legal systems in different geographies. The paper begins by providing a high-level overview of {AI} technologies in the legal field and showcasing recent research advancements in {LLMs}, followed by practical implementations of legal {LLMs}. Two databases (i.e., {SCOPUS} and Web of Science) were considered alongside additional related studies that met our selection criteria. We used the {PRISMA} for Scoping Reviews ({PRISMA}-{ScR}) guidelines as the methodology to extract relevant studies and report our findings. The paper discusses and analyses the limitations and challenges faced by legal {LLMs}, including issues related to data, algorithms, and judicial practices. Moreover, we examine the extent to which such systems can be effectively deployed. The paper summarizes recommendations and future directions to address challenges, aiming to help stakeholders overcome limitations and integrate legal {LLMs} into the judicial system.},
	pages = {662},
	number = {11},
	journaltitle = {Information},
	author = {Padiu, Bogdan and Iacob, Radu and Rebedea, Traian and Dascalu, Mihai},
	urldate = {2025-04-15},
	date = {2024-11},
	langid = {english},
	note = {Number: 11
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {judicial data, large language models ({LLMs}), legal, legal datasets, natural language processing, scoping review},
	file = {Full Text PDF:C\:\\Users\\aless\\Zotero\\storage\\8ERGSVBD\\Padiu et al. - 2024 - To What Extent Have LLMs Reshaped the Legal Domain So Far A Scoping Literature Review.pdf:application/pdf},
}

@misc{vatsal_survey_2024,
	title = {A Survey of Prompt Engineering Methods in Large Language Models for Different {NLP} Tasks},
	url = {http://arxiv.org/abs/2407.12994},
	doi = {10.48550/arXiv.2407.12994},
	abstract = {Large language models ({LLMs}) have shown remarkable performance on many different Natural Language Processing ({NLP}) tasks. Prompt engineering plays a key role in adding more to the already existing abilities of {LLMs} to achieve significant performance gains on various {NLP} tasks. Prompt engineering requires composing natural language instructions called prompts to elicit knowledge from {LLMs} in a structured way. Unlike previous state-of-the-art ({SoTA}) models, prompt engineering does not require extensive parameter re-training or fine-tuning based on the given {NLP} task and thus solely operates on the embedded knowledge of {LLMs}. Additionally, {LLM} enthusiasts can intelligently extract {LLMs}' knowledge through a basic natural language conversational exchange or prompt engineering, allowing more and more people even without deep mathematical machine learning background to experiment with {LLMs}. With prompt engineering gaining popularity in the last two years, researchers have come up with numerous engineering techniques around designing prompts to improve accuracy of information extraction from the {LLMs}. In this paper, we summarize different prompting techniques and club them together based on different {NLP} tasks that they have been used for. We further granularly highlight the performance of these prompting strategies on various datasets belonging to that {NLP} task, talk about the corresponding {LLMs} used, present a taxonomy diagram and discuss the possible {SoTA} for specific datasets. In total, we read and present a survey of 44 research papers which talk about 39 different prompting methods on 29 different {NLP} tasks of which most of them have been published in the last two years.},
	number = {{arXiv}:2407.12994},
	publisher = {{arXiv}},
	author = {Vatsal, Shubham and Dubey, Harsh},
	urldate = {2025-04-15},
	date = {2024-07-17},
	eprinttype = {arxiv},
	eprint = {2407.12994 [cs]},
	note = {version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\aless\\Zotero\\storage\\67XSTZ3R\\Vatsal e Dubey - 2024 - A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks.pdf:application/pdf;Snapshot:C\:\\Users\\aless\\Zotero\\storage\\EL7F3IRT\\2407.html:text/html},
}

@misc{chen_unleashing_2024,
	title = {Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review},
	url = {http://arxiv.org/abs/2310.14735},
	doi = {10.48550/arXiv.2310.14735},
	shorttitle = {Unleashing the potential of prompt engineering in Large Language Models},
	abstract = {This comprehensive review delves into the pivotal role of prompt engineering in unleashing the capabilities of Large Language Models ({LLMs}). The development of Artificial Intelligence ({AI}), from its inception in the 1950s to the emergence of advanced neural networks and deep learning architectures, has made a breakthrough in {LLMs}, with models such as {GPT}-4o and Claude-3, and in Vision-Language Models ({VLMs}), with models such as {CLIP} and {ALIGN}. Prompt engineering is the process of structuring inputs, which has emerged as a crucial technique to maximize the utility and accuracy of these models. This paper explores both foundational and advanced methodologies of prompt engineering, including techniques such as self-consistency, chain-of-thought, and generated knowledge, which significantly enhance model performance. Additionally, it examines the prompt method of {VLMs} through innovative approaches such as Context Optimization ({CoOp}), Conditional Context Optimization ({CoCoOp}), and Multimodal Prompt Learning ({MaPLe}). Critical to this discussion is the aspect of {AI} security, particularly adversarial attacks that exploit vulnerabilities in prompt engineering. Strategies to mitigate these risks and enhance model robustness are thoroughly reviewed. The evaluation of prompt methods is also addressed, through both subjective and objective metrics, ensuring a robust analysis of their efficacy. This review also reflects the essential role of prompt engineering in advancing {AI} capabilities, providing a structured framework for future research and application.},
	number = {{arXiv}:2310.14735},
	publisher = {{arXiv}},
	author = {Chen, Banghao and Zhang, Zhaofeng and Langrené, Nicolas and Zhu, Shengxin},
	urldate = {2025-04-15},
	date = {2024-09-05},
	eprinttype = {arxiv},
	eprint = {2310.14735 [cs]},
	note = {version: 5},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\aless\\Zotero\\storage\\GHK8IQZH\\Chen et al. - 2024 - Unleashing the potential of prompt engineering in Large Language Models a comprehensive review.pdf:application/pdf;Snapshot:C\:\\Users\\aless\\Zotero\\storage\\QNS7DAFP\\2310.html:text/html},
}

@misc{chen_survey_2024,
	title = {A Survey on Large Language Models for Critical Societal Domains: Finance, Healthcare, and Law},
	url = {http://arxiv.org/abs/2405.01769},
	doi = {10.48550/arXiv.2405.01769},
	shorttitle = {A Survey on Large Language Models for Critical Societal Domains},
	abstract = {In the fast-evolving domain of artificial intelligence, large language models ({LLMs}) such as {GPT}-3 and {GPT}-4 are revolutionizing the landscapes of finance, healthcare, and law: domains characterized by their reliance on professional expertise, challenging data acquisition, high-stakes, and stringent regulatory compliance. This survey offers a detailed exploration of the methodologies, applications, challenges, and forward-looking opportunities of {LLMs} within these high-stakes sectors. We highlight the instrumental role of {LLMs} in enhancing diagnostic and treatment methodologies in healthcare, innovating financial analytics, and refining legal interpretation and compliance strategies. Moreover, we critically examine the ethics for {LLM} applications in these fields, pointing out the existing ethical concerns and the need for transparent, fair, and robust {AI} systems that respect regulatory norms. By presenting a thorough review of current literature and practical applications, we showcase the transformative impact of {LLMs}, and outline the imperative for interdisciplinary cooperation, methodological advancements, and ethical vigilance. Through this lens, we aim to spark dialogue and inspire future research dedicated to maximizing the benefits of {LLMs} while mitigating their risks in these precision-dependent sectors. To facilitate future research on {LLMs} in these critical societal domains, we also initiate a reading list that tracks the latest advancements under this topic, which will be continually updated: {\textbackslash}url\{https://github.com/czyssrs/{LLM}\_X\_papers\}.},
	number = {{arXiv}:2405.01769},
	publisher = {{arXiv}},
	author = {Chen, Zhiyu Zoey and Ma, Jing and Zhang, Xinlu and Hao, Nan and Yan, An and Nourbakhsh, Armineh and Yang, Xianjun and {McAuley}, Julian and Petzold, Linda and Wang, William Yang},
	urldate = {2025-04-15},
	date = {2024-05-02},
	eprinttype = {arxiv},
	eprint = {2405.01769 [cs]},
	note = {version: 1},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\aless\\Zotero\\storage\\XN24JNZP\\Chen et al. - 2024 - A Survey on Large Language Models for Critical Societal Domains Finance, Healthcare, and Law.pdf:application/pdf;Snapshot:C\:\\Users\\aless\\Zotero\\storage\\9I4KLGLP\\2405.html:text/html},
}

@article{borissov_potential_2024,
	title = {{POTENTIAL} {FOR} {USING} {ARTIFICAL} {INTELLIGENCE} {IN} {PUBLIC} {ADMINISTRATION}},
	volume = {12},
	rights = {Copyright (c) 2024 {ECONOMICS} - {INNOVATIVE} {AND} {ECONOMICS} {RESEARCH} {JOURNAL}},
	issn = {2303-5013},
	url = {https://economicsrs.com/index.php/eier/article/view/557},
	doi = {10.2478/eoik-2024-0034},
	abstract = {Artificial intelligence has become a defining technology for the last decade and possibly the next few. Every day, new and new applications are created based on large language models ({LLM}), a little hastily called artificial intelligence ({AI}). This reveals new and new opportunities for their use in various spheres of public life. Public administration, despite its inherent conservatism, is also one such area where {AI} can be used to enhance its administrative capacity and citizens’ satisfaction with administrative services. The aim of this article is to address the possibilities of using {AI} in public sector organizations and to reveal the limitations that hinder it. In this sense, the object of the research is the Bulgarian state institutions, and the subject - the application of {AI} in their work. A study was conducted that shows that the employees in the Bulgarian state administration still do not know the possibilities of {AI} and how to use it in their work. Abstention is due to both ignorance and lack of regulation about what apps can be used where, as well as fear of possible risks. The report presents the possibilities of using some {AI}-based applications in the implementation of basic work processes in administrations and justifies the need to introduce strict regulations for this. The author’s hypothesis will defend the claim that the Bulgarian administration does not know well the possibilities of digital transformation and {AI}, through which their work and efficiency can be improved.},
	pages = {409--423},
	number = {3},
	journaltitle = {{ECONOMICS} - {INNOVATIVE} {AND} {ECONOMICS} {RESEARCH} {JOURNAL}},
	author = {Borissov, Borislav and Hristozov, Yanko},
	urldate = {2025-04-15},
	date = {2024-11-27},
	langid = {english},
	note = {Number: 3},
	keywords = {Public Administration},
	file = {Full Text PDF:C\:\\Users\\aless\\Zotero\\storage\\6D7WEYFM\\Borissov e Hristozov - 2024 - POTENTIAL FOR USING ARTIFICAL INTELLIGENCE IN PUBLIC ADMINISTRATION.pdf:application/pdf},
}

@article{holzhausen_legal_2024,
	title = {Legal Accountability and Ethical Considerations for Outcomes Driven by Artificial Intelligence in Business Operations},
	volume = {8},
	issn = {2549-0680},
	url = {https://ojs.unud.ac.id/index.php/UJLC/article/view/108567},
	doi = {10.24843/UJLC.2024.v08.i01.p01},
	abstract = {This paper critically examines the integration of Artificial Intelligence ({AI}) into business operations, focusing on the challenges of legal accountability and ethical considerations. It first traces the development of {AI} and its transformative impact on commerce, providing a basis for examining the key ethical and responsibility challenges. The paper presents research findings that highlight the complexity of assigning responsibility for {AI}-generated outcomes and discusses the different approaches in national and international legal frameworks for {AI}. It emphasizes the need for clear legal structures and ethical guidelines to govern the role of {AI} in business and society. The paper concludes by highlighting the importance of harmonized global frameworks to ensure the responsible integration of {AI}, addressing both theoretical and policy implications. The findings point to a significant shift in legal trends and societal impacts due to {AI} and emphasize the urgent need for ethical deployment to prevent the reinforcement of societal biases.},
	pages = {1},
	number = {1},
	journaltitle = {Udayana Journal of Law and Culture},
	shortjournal = {{UJLC}},
	author = {Holzhausen, Matthias},
	urldate = {2025-04-15},
	date = {2024-01-30},
	langid = {english},
	file = {PDF:C\:\\Users\\aless\\Zotero\\storage\\RTELIEP6\\Holzhausen - 2024 - Legal Accountability and Ethical Considerations for Outcomes Driven by Artificial Intelligence in Bu.pdf:application/pdf},
}

@misc{jiao_navigating_2025,
	title = {Navigating {LLM} Ethics: Advancements, Challenges, and Future Directions},
	url = {http://arxiv.org/abs/2406.18841},
	doi = {10.48550/arXiv.2406.18841},
	shorttitle = {Navigating {LLM} Ethics},
	abstract = {This study addresses ethical issues surrounding Large Language Models ({LLMs}) within the field of artificial intelligence. It explores the common ethical challenges posed by both {LLMs} and other {AI} systems, such as privacy and fairness, as well as ethical challenges uniquely arising from {LLMs}. It highlights challenges such as hallucination, verifiable accountability, and decoding censorship complexity, which are unique to {LLMs} and distinct from those encountered in traditional {AI} systems. The study underscores the need to tackle these complexities to ensure accountability, reduce biases, and enhance transparency in the influential role that {LLMs} play in shaping information dissemination. It proposes mitigation strategies and future directions for {LLM} ethics, advocating for interdisciplinary collaboration. It recommends ethical frameworks tailored to specific domains and dynamic auditing systems adapted to diverse contexts. This roadmap aims to guide responsible development and integration of {LLMs}, envisioning a future where ethical considerations govern {AI} advancements in society.},
	number = {{arXiv}:2406.18841},
	publisher = {{arXiv}},
	author = {Jiao, Junfeng and Afroogh, Saleh and Xu, Yiming and Phillips, Connor},
	urldate = {2025-04-15},
	date = {2025-03-18},
	eprinttype = {arxiv},
	eprint = {2406.18841 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
	file = {Preprint PDF:C\:\\Users\\aless\\Zotero\\storage\\YSGHD7GB\\Jiao et al. - 2025 - Navigating LLM Ethics Advancements, Challenges, and Future Directions.pdf:application/pdf;Snapshot:C\:\\Users\\aless\\Zotero\\storage\\WIKAUVFA\\2406.html:text/html},
}

@article{christoforaki_ai_2022,
	title = {{AI} Ethics—A Bird’s Eye View},
	volume = {12},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/12/9/4130},
	doi = {10.3390/app12094130},
	abstract = {The explosion of data-driven applications using Artiﬁcial Intelligence ({AI}) in recent years has given rise to a variety of ethical issues regarding data collection, annotation, and processing using mostly opaque algorithms, as well as the interpretation and employment of the results of the {AI} pipeline. The ubiquity of {AI} applications negatively impacts a variety of sensitive areas, ranging from discrimination against vulnerable populations to privacy invasion and the environmental cost that these algorithms entail, and puts into focus on the ever present domain of {AI} ethics. In this review article we present a bird’s eye view approach of the {AI} ethics landscape, starting from a historical point of view, examining the moral issues that were introduced by big datasets and the application of non-symbolic {AI} algorithms, the normative approaches (principles and guidelines) to these issues and the ensuing criticism, as well as the actualization of these principles within the proposed frameworks. Subsequently, we focus on the concept of responsibility, both as personal responsibility of the {AI} practitioners and sustainability, meaning the promotion of beneﬁcence for both the society and the domain, and the role of professional certiﬁcation and education in averting unethical choices. Finally, we conclude with indicating the multidisciplinary nature of {AI} ethics and suggesting future challenges.},
	pages = {4130},
	number = {9},
	journaltitle = {Applied Sciences},
	shortjournal = {Applied Sciences},
	author = {Christoforaki, Maria and Beyan, Oya},
	urldate = {2025-04-15},
	date = {2022-04-20},
	langid = {english},
	file = {PDF:C\:\\Users\\aless\\Zotero\\storage\\Y95KAW57\\Christoforaki e Beyan - 2022 - AI Ethics—A Bird’s Eye View.pdf:application/pdf},
}

@article{sobrino-garcia_artificial_2021,
	title = {Artificial Intelligence Risks and Challenges in the Spanish Public Administration: An Exploratory Analysis through Expert Judgements},
	volume = {11},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2076-3387},
	url = {https://www.mdpi.com/2076-3387/11/3/102},
	doi = {10.3390/admsci11030102},
	shorttitle = {Artificial Intelligence Risks and Challenges in the Spanish Public Administration},
	abstract = {The expanding use of artiﬁcial intelligence ({AI}) in public administration is generating numerous opportunities for governments. Current Spanish regulations have established electronic administration and support the expansion and implementation of this new technology, but they may not be adapted to the legal needs caused by {AI}. Consequently, this research aims to identify the risks associated with {AI} uses in Spanish public administration and if the legal mechanisms can solve them. We answer these questions by employing a qualitative research approach, conducting semi-structured interviews with several experts in the matter. Despite the beneﬁts that this technology may involve, throughout this research we can conﬁrm that the use of artiﬁcial intelligence can generate several problems such as opacity, legal uncertainty, biases, or breaches of personal data protection. The mechanisms already provided by Spanish law are not enough to avoid these risks as they have not been designed to face the use of artiﬁcial intelligence in public administration. In addition, a homogeneous legal deﬁnition of {AI} needs to be established.},
	pages = {102},
	number = {3},
	journaltitle = {Administrative Sciences},
	shortjournal = {Administrative Sciences},
	author = {Sobrino-García, Itziar},
	urldate = {2025-04-15},
	date = {2021-09-16},
	langid = {english},
	file = {PDF:C\:\\Users\\aless\\Zotero\\storage\\9DM2A88W\\Sobrino-García - 2021 - Artificial Intelligence Risks and Challenges in the Spanish Public Administration An Exploratory An.pdf:application/pdf},
}

@misc{overton_tampering_2025,
	title = {{TaMPERing} with Large Language Models: A Field Guide for using Generative {AI} in Public Administration Research},
	url = {http://arxiv.org/abs/2504.01037},
	doi = {10.48550/arXiv.2504.01037},
	shorttitle = {{TaMPERing} with Large Language Models},
	abstract = {The integration of Large Language Models ({LLMs}) into social science research presents transformative opportunities for advancing scientific inquiry, particularly in public administration ({PA}). However, the absence of standardized methodologies for using {LLMs} poses significant challenges for ensuring transparency, reproducibility, and replicability. This manuscript introduces the {TaMPER} framework-a structured methodology organized around five critical decision points: Task, Model, Prompt, Evaluation, and Reporting. The {TaMPER} framework provides scholars with a systematic approach to leveraging {LLMs} effectively while addressing key challenges such as model variability, prompt design, evaluation protocols, and transparent reporting practices.},
	number = {{arXiv}:2504.01037},
	publisher = {{arXiv}},
	author = {Overton, Michael and Robison, Barrie and Sheneman, Lucas},
	urldate = {2025-04-15},
	date = {2025-03-30},
	eprinttype = {arxiv},
	eprint = {2504.01037 [cs]},
	note = {version: 1},
	keywords = {Computer Science - Computers and Society},
	file = {Preprint PDF:C\:\\Users\\aless\\Zotero\\storage\\K6E8H5GN\\Overton et al. - 2025 - TaMPERing with Large Language Models A Field Guide for using Generative AI in Public Administration.pdf:application/pdf;Snapshot:C\:\\Users\\aless\\Zotero\\storage\\L2I5M62E\\2504.html:text/html},
}

@article{babsek_artificial_2025,
	title = {Artificial Intelligence Adoption in Public Administration: An Overview of Top-Cited Articles and Practical Applications},
	volume = {6},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2673-2688},
	url = {https://www.mdpi.com/2673-2688/6/3/44},
	doi = {10.3390/ai6030044},
	shorttitle = {Artificial Intelligence Adoption in Public Administration},
	abstract = {Background: The adoption of artificial intelligence ({AI}) in public administration ({PA}) has the potential to enhance transparency, efficiency, and responsiveness, ultimately creating greater public value. However, the integration of {AI} into {PA} faces challenges, including conceptual ambiguities and limited knowledge of the practical applications. This study addresses these gaps by offering an overview and categorization of {AI} research and applications in {PA}. Methods: Using a dataset of 3149 documents from the Scopus database, this study identifies the top 200 most-cited articles based on citation per year. It conducts descriptive and content analyses to identify the existing state, applications, and challenges regarding {AI} adoption. Additionally, selected {AI} use cases from the European Commission’s database are categorized, focusing on their contributions to public value. The analysis centers on three governance dimensions: internal processes, service delivery, and policymaking. Results: The findings provide a categorized understanding of {AI} concepts, types, and applications in {PA}, alongside a discussion of best practices and challenges. Conclusion: This study serves as a resource for researchers seeking a comprehensive overview of the current state of {AI} in {PA} and offers policymakers and practitioners insights into leveraging {AI} technologies to improve service delivery and operational efficiency.},
	pages = {44},
	number = {3},
	journaltitle = {{AI}},
	author = {Babšek, Matej and Ravšelj, Dejan and Umek, Lan and Aristovnik, Aleksander},
	urldate = {2025-04-15},
	date = {2025-03},
	langid = {english},
	note = {Number: 3
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {adoption, artificial intelligence, content analysis, descriptive study, practical implementation, public administration},
	file = {Full Text PDF:C\:\\Users\\aless\\Zotero\\storage\\LGPM4VW8\\Babšek et al. - 2025 - Artificial Intelligence Adoption in Public Administration An Overview of Top-Cited Articles and Pra.pdf:application/pdf},
}
